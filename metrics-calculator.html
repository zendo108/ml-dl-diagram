<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Metrics Calculator & Analysis Tool</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
        }
        input {
            margin: 5px;
            padding: 5px;
            font-size: 16px;
        }
        .section {
            margin-bottom: 30px;
        }
        .output {
            margin-top: 20px;
        }
        .explanation {
            margin-top: 20px;
            font-weight: bold;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
        }
        table, th, td {
            border: 1px solid #ddd;
        }
        th, td {
            padding: 10px;
            text-align: center;
        }
        th {
            background-color: #f4f4f9;
        }
    </style>
</head>
<body>
    <h1>Metrics Calculator & Analysis Tool</h1>

    <!-- Section 1: Confusion Matrix Calculator -->
    <div class="section">
        <h2>Confusion Matrix Calculator</h2>
        <p>Enter the confusion matrix values:</p>
        <label>True Positives (TP): <input id="tp" type="number"></label><br>
        <label>False Positives (FP): <input id="fp" type="number"></label><br>
        <label>True Negatives (TN): <input id="tn" type="number"></label><br>
        <label>False Negatives (FN): <input id="fn" type="number"></label><br>
        <button onclick="calculateConfusionMetrics()">Calculate Metrics</button>
        <div class="output" id="confusion-output"></div>
    </div>

    <!-- Section 2: Metrics Interpretation Tool -->
    <div class="section">
        <h2>Metrics Interpretation Tool</h2>
        <p>Pre-filled from the confusion matrix calculator. Modify if necessary:</p>
        <label>Accuracy (%): <input id="accuracy" type="number" step="0.01"></label><br>
        <label>Sensitivity (Recall) (%): <input id="sensitivity" type="number" step="0.01"></label><br>
        <label>Specificity (%): <input id="specificity" type="number" step="0.01"></label><br>
        <label>Precision (%): <input id="precision" type="number" step="0.01"></label><br>
        <label>F1 Score (%): <input id="f1" type="number" step="0.01"></label><br>
        <button onclick="analyzePerformance()">Analyze</button>
        <div class="output" id="interpretation-output"></div>
        <div class="explanation" id="interpretation-explanation"></div>
    </div>

    <!-- Section 3: Metric Ranges Table -->
    <div class="section">
        <h2>Metric Interpretation Table</h2>
        <table>
            <thead>
                <tr>
                    <th>Metric</th>
                    <th>Low (0-50%)</th>
                    <th>Medium (51-80%)</th>
                    <th>High (81-100%)</th>
                    <th>Interpretation</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Accuracy</td>
                    <td>Poor overall correctness</td>
                    <td>Decent predictions</td>
                    <td>Excellent overall correctness</td>
                    <td>May be misleading in imbalanced datasets. Check other metrics for deeper insight.</td>
                </tr>
                <tr>
                    <td>Sensitivity (Recall)</td>
                    <td>Many false negatives</td>
                    <td>Some false negatives</td>
                    <td>Few false negatives</td>
                    <td>High sensitivity ensures positives are rarely missed (useful in medical or fraud detection).</td>
                </tr>
                <tr>
                    <td>Specificity</td>
                    <td>Many false positives</td>
                    <td>Some false positives</td>
                    <td>Few false positives</td>
                    <td>High specificity ensures negatives are rarely misclassified (useful in spam filtering).</td>
                </tr>
                <tr>
                    <td>Precision</td>
                    <td>Many false positives</td>
                    <td>Decent precision</td>
                    <td>Few false positives</td>
                    <td>High precision is critical when false positives are costly (e.g., spam detection).</td>
                </tr>
                <tr>
                    <td>F1 Score</td>
                    <td>Imbalanced precision/recall</td>
                    <td>Good balance</td>
                    <td>Strong balance</td>
                    <td>High F1 indicates the model balances precision and recall well.</td>
                </tr>
            </tbody>
        </table>
    </div>

    <script>
        // Confusion Matrix Calculator
        function calculateConfusionMetrics() {
            const tp = parseInt(document.getElementById("tp").value || 0);
            const fp = parseInt(document.getElementById("fp").value || 0);
            const tn = parseInt(document.getElementById("tn").value || 0);
            const fn = parseInt(document.getElementById("fn").value || 0);

            // Metrics calculations
            const accuracy = ((tp + tn) / (tp + fp + tn + fn) * 100).toFixed(2);
            const precision = (tp / (tp + fp) * 100).toFixed(2);
            const recall = (tp / (tp + fn) * 100).toFixed(2); // Sensitivity
            const specificity = (tn / (tn + fp) * 100).toFixed(2);
            const f1 = (2 * (precision * recall) / (parseFloat(precision) + parseFloat(recall))).toFixed(2);

            // Display results
            const outputDiv = document.getElementById("confusion-output");
            outputDiv.innerHTML = `
                <p><strong>Metrics Calculated:</strong></p>
                <ul>
                    <li>Accuracy: ${accuracy}%</li>
                    <li>Precision: ${precision}%</li>
                    <li>Recall (Sensitivity): ${recall}%</li>
                    <li>Specificity: ${specificity}%</li>
                    <li>F1 Score: ${f1}%</li>
                </ul>
            `;

            // Autofill interpretation fields
            document.getElementById("accuracy").value = accuracy;
            document.getElementById("sensitivity").value = recall;
            document.getElementById("specificity").value = specificity;
            document.getElementById("precision").value = precision;
            document.getElementById("f1").value = f1;
        }

        // Metrics Interpretation Tool
        function analyzePerformance() {
            const accuracy = parseFloat(document.getElementById("accuracy").value || 0);
            const sensitivity = parseFloat(document.getElementById("sensitivity").value || 0);
            const specificity = parseFloat(document.getElementById("specificity").value || 0);
            const precision = parseFloat(document.getElementById("precision").value || 0);
            const f1 = parseFloat(document.getElementById("f1").value || 0);

            const outputDiv = document.getElementById("interpretation-output");
            const explanationDiv = document.getElementById("interpretation-explanation");

            // Display entered values
            outputDiv.innerHTML = `
                <p><strong>Entered Metrics:</strong></p>
                <ul>
                    <li>Accuracy: ${accuracy}%</li>
                    <li>Sensitivity (Recall): ${sensitivity}%</li>
                    <li>Specificity: ${specificity}%</li>
                    <li>Precision: ${precision}%</li>
                    <li>F1 Score: ${f1}%</li>
                </ul>
            `;

            // Analyze performance
            let explanation = "";

            if (accuracy < 50) {
                explanation += "The model's accuracy is below acceptable levels. This could indicate random predictions or poor feature selection.<br>";
            }

            if (sensitivity >= 95 && specificity <= 10) {
                explanation += "The model has high sensitivity but very low specificity. This suggests it is predicting almost everything as positive, leading to a high rate of false positives.<br>";
                explanation += "For improvement, try balancing the dataset, adjusting decision thresholds, or using algorithms robust to class imbalance like Random Forest or XGBoost.<br>";
            }

            if (sensitivity < 50) {
                explanation += "Low sensitivity indicates the model is missing many positive cases (high false negatives). Consider improving the feature set or using more complex models.<br>";
            }

            if (specificity < 50 && specificity > 10) {
                explanation += "Specificity is low, which means the model struggles to identify true negatives. Investigate if the data is imbalanced or if the features used are biased toward positive cases.<br>";
            }

            if (sensitivity > 90 && specificity > 90) {
                explanation += "The model demonstrates good sensitivity and specificity, indicating balanced performance in identifying positives and negatives.<br>";
            }

            explanationDiv.innerHTML = explanation || "The metrics are within acceptable ranges. No major performance issues detected.";
        }
    </script>
</body>
</html>
